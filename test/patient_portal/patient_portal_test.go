package patient_portal

import (
	"fmt"
	"testing"

	"github.com/hash-d/frame2/pkg/frames/f2k8s/disruptor"
	disruptor3 "github.com/hash-d/frame2/pkg/frames/f2ocp/disruptor"
	"github.com/hash-d/frame2/pkg/frames/f2skupper1"
	disruptor2 "github.com/hash-d/frame2/pkg/frames/f2skupper1/disruptor"
	disruptor4 "github.com/hash-d/frame2/pkg/frames/f2skupper1/disruptor"
	"github.com/hash-d/frame2/pkg/frames/f2skupper1/f2sk1deploy"
	"github.com/hash-d/frame2/pkg/frames/f2skupper1/f2sk1environment"

	frame2 "github.com/hash-d/frame2/pkg"
	"github.com/hash-d/frame2/pkg/disruptors"
	"github.com/hash-d/frame2/pkg/frames/f2k8s"
	"gotest.tools/assert"
)

/*
func init() {
	null, _ := os.Open(os.DevNull)
	os.Stdout = null
	//	os.Stderr = null
	log.SetOutput(null)
}
*/

func TestPatientPortalTemplate(t *testing.T) {
	// Every frame2 test has a root frame2.Run, that keeps the reference
	// to the t *testing.T
	r := &frame2.Run{
		T: t,
	}
	// This will print the monitor's report at the end of the run
	defer r.Report()

	// Every test should have this; it will re-run the final validators,
	// and may be used by disruptors as well
	defer r.Finalize()

	// Disruptors supported by the test need to be listed here.  This may
	// change in the future, in a way that at least some of the disruptors
	// will be allowed to all tests.
	r.AllowDisruptors(
		[]frame2.Disruptor{
			&disruptor2.UpgradeAndFinalize{},
			&disruptor2.MixedVersionVan{},
			&disruptor3.DeploymentConfigBlindly{},
			&disruptor2.NoConsole{},
			&disruptor2.NoFlowCollector{},
			&disruptor2.NoHttp{},
			&disruptor2.ConsoleOnAll{},
			&disruptor2.FlowCollectorOnAll{},
			&disruptors.MinAllows{},
			&disruptor2.SkipManifestCheck{},
			&disruptor.PodSecurityAdmission{},
			&disruptor.PSADeployment{},
			&disruptor2.EdgeOnPrivate{},
			&disruptor4.AlternateSkupper{},
			// &disruptors.ClusterPermissions{},
		},
	)

	// This is run on the next block, the first phase, called setup.
	// It has its own variable (instead of being declared with the
	// Step), because it allows us to get information from it further
	// down (specifically, we get the references to the namespaces
	// created by this environment call).
	//
	// As an environment action, this will create namespaces, install
	// Skupper on them, link them, install the application and expose
	// it via Skupper.
	env := f2sk1environment.PatientPortalDefault{
		AutoTearDown:  true,
		EnableConsole: true,
	}

	// The actual test steps run on Phases.  Each phase has a list of
	// steps to be executed.  The steps may have a single modify action,
	// a list of validators, and other configurations.
	setup := frame2.Phase{
		// Phases must always point to the runner, so they run as
		// connected to the test.  In the future, this will change
		// to Run(Phase), instead, so it gets harder to forget it.
		Runner: r,
		// In the future, this may become mandatory (ie, the test
		// will fail if Doc is not defined).  Grepping the logs for
		// '[Dd]oc[: ]' will give you a summary of what's going on.
		// That format may change in the future, too.
		Doc: "Deploy Patient Portal on the default topology",
		Setup: []frame2.Step{
			{
				Doc:    "Deploy Patient Portal",
				Modify: &env,
			},
		},
	}
	// We use assert on the highest level part of the test only (as on
	// other parts we have no access to it).
	assert.Assert(t, setup.Run())

	// Here we get the reference to the namespaces generated by the
	// environment action, so we can use them later on.
	front_ns, err := env.TopoReturn.Get(f2k8s.Public, 1)
	if err != nil {
		t.Fatalf(fmt.Sprintf("failed to get pub-1: %v", err))
	}
	back_ns, err := env.TopoReturn.Get(f2k8s.Private, 1)
	if err != nil {
		t.Fatalf(fmt.Sprintf("failed to get prv-1: %v", err))
	}

	// In case you're running with disruptors, the monitors not only inform when connectivity
	// was disrupted, but also help with applications re-establishing their connections (on
	// a pool, for example) after a disruption, and before the actual test.
	monitorPhase := frame2.Phase{
		Runner: r,
		Setup: []frame2.Step{
			{
				Doc: "Install Patient Portal monitors",
				Modify: &frame2.DefaultMonitor{
					Validators: map[string]frame2.Validator{
						"frontend-health": &f2sk1deploy.PatientFrontendHealth{
							Namespace: front_ns,
						},
						"database-ping": &f2sk1deploy.PatientDbPing{
							Namespace: front_ns,
						},
						"payment-token": &f2sk1deploy.PatientValidatePayment{
							Namespace: front_ns,
						},
					},
				},
			},
		},
	}
	assert.Assert(t, monitorPhase.Run())

	main := frame2.Phase{
		Runner: r,
		Name:   "sample-tests",
		Doc:    "Replace these by your modifications and tests",
		MainSteps: []frame2.Step{
			{
				Doc: "Check PatientPortal components",
				Validators: []frame2.Validator{
					&f2sk1deploy.PatientValidatePayment{
						Namespace: front_ns,
					},
					&f2sk1deploy.PatientFrontendHealth{
						Namespace: front_ns,
					},
					&f2sk1deploy.PatientDbPing{
						Namespace: front_ns,
					},
					&f2sk1deploy.PatientValidatePayment{
						Namespace: back_ns,
					},
					// frontend cannot be tested from backend, as not skupper-exposed
					//
					// db currently cannot be tested from backend, as it depends on a
					// frontend deployment, to use its pg_isalive
				},
				ValidatorFinal: true,
			}, {
				Doc: "check status commands",
				Validators: []frame2.Validator{
					&f2skupper1.CliSkupper{
						F2Namespace: front_ns,
						Args:        []string{"version"},
					},
					&f2skupper1.CliSkupper{
						F2Namespace: front_ns,
						Args:        []string{"status"},
					},
					&f2skupper1.CliSkupper{
						F2Namespace: front_ns,
						Args:        []string{"network", "status"},
					},
					&f2skupper1.CliSkupper{
						F2Namespace: front_ns,
						Args:        []string{"service", "status"},
					},
					&f2skupper1.CliSkupper{
						F2Namespace: back_ns,
						Args:        []string{"version"},
					},
					&f2skupper1.CliSkupper{
						F2Namespace: back_ns,
						Args:        []string{"status"},
					},
					&f2skupper1.CliSkupper{
						F2Namespace: back_ns,
						Args:        []string{"network", "status"},
					},
					&f2skupper1.CliSkupper{
						F2Namespace: back_ns,
						Args:        []string{"service", "status"},
					},
				},
				ValidatorFinal: true,
			},
		},
	}

	assert.Assert(t, main.Run())

	// Teardown: for the template, all tear down is automatic.
	// If specific tear downs from the main steps are required,
	// create a new phase and specify them.

}
